# 3. Flink输入输出-kafka
 
## 功能描述
从kafka的student topic中读取数据过滤出年龄大于16的记录写入到stuout中。



## 环境准备

需要一个运行中的kafka,要求kafka的版本>=1.0.0。低版本的kafka需要切换到相应版本的依赖。

pom.xml加入依赖
```
<dependency>
	<groupId>org.apache.flink</groupId>
	<artifactId>flink-connector-kafka_${scala.binary.version}</artifactId>
	<version>${flink.version}</version>
</dependency>

<dependency>
	<groupId>org.apache.flink</groupId>
	<artifactId>flink-json</artifactId>
	<version>${flink.version}</version>
</dependency>
```

## 生成测试数据 
```
import org.apache.flink.table.runtime.util.JsonUtils;
import org.apache.kafka.clients.producer.*;
import java.util.HashMap;
import java.util.Map;
import java.util.Properties;
import java.util.Random;

/**
 * 生成一些测试数据放入到kafka 的 student 中
 */
public class KafkaGenerateData {

    public static void main(String[] args) throws Exception{

        Properties properties = new Properties();
        properties.setProperty("bootstrap.servers", "127.0.0.1:9092");
        properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        properties.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        final String topic = "student";

        Producer<String, String> producer = new KafkaProducer<>(properties);

        final Random random = new Random();
        for (int i = 0; i < 100; i++){
            Thread.sleep(300);

            Map<String, Object> map = new HashMap<>();
            map.put("name", "xiao-"+i);
            map.put("age", 10 + random.nextInt(10));
            map.put("class", "class-"+ random.nextInt(5));

            // 发送json 字符串
            producer.send(new ProducerRecord<String, String>(topic,
                    JsonUtils.MAPPER.writeValueAsString(map)));
        }

        producer.flush();
        producer.close();

    }
}
```
> 执行该生成数据的代码，向kafka中写入数据。

## DataStream Api方式

```
import org.apache.flink.api.common.functions.FilterFunction;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.api.java.typeutils.RowTypeInfo;
import org.apache.flink.core.fs.FileSystem;
import org.apache.flink.formats.json.JsonRowDeserializationSchema;
import org.apache.flink.formats.json.JsonRowSerializationSchema;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;
import org.apache.flink.streaming.connectors.kafka.KafkaSerializationSchema;
import org.apache.flink.types.Row;
import org.apache.kafka.clients.producer.ProducerRecord;
import javax.annotation.Nullable;
import java.util.Properties;

/**
 * 使用 Stream Api方式从 kafka 中读写 数据
 */
public class KafkaStreamApi {

    public static void main(String[] args) throws Exception{
        // 获取运行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        // 设置并发度为1 可以不设
        env.setParallelism(1);

        // kafka 连接信息
        Properties properties = new Properties();
        properties.setProperty("bootstrap.servers", "127.0.0.1:9092");
        properties.setProperty("group.id", "KafkaStreamApi");

        // 反序列化 把byte[]转为 Row
        JsonRowDeserializationSchema schema = new JsonRowDeserializationSchema.Builder(new RowTypeInfo(
                new TypeInformation[]{Types.STRING, Types.INT, Types.STRING},
                new String[]{"name", "age", "class"}
        ))
                .failOnMissingField()
                .build();

        // 创建 kafka 消费者
        FlinkKafkaConsumer<Row> input = new FlinkKafkaConsumer<Row>("student", schema , properties);
        input.setStartFromEarliest();

        // 序列化 把 Row 转为 byte[]
        JsonRowSerializationSchema outSchema = new JsonRowSerializationSchema
                .Builder(schema.getProducedType()).build();

        // 创建 kafka 生产者
        FlinkKafkaProducer<Row> output = new FlinkKafkaProducer<Row>("stuout",
                new KafkaSerializationSchema<Row>() {
                    @Override
                    public ProducerRecord<byte[], byte[]> serialize(Row element, @Nullable Long timestamp) {
                        return new ProducerRecord<byte[], byte[]>("stuout", outSchema.serialize(element));
                    }
                },
                properties, FlinkKafkaProducer.Semantic.AT_LEAST_ONCE);


        // 1. 读取 kafka 的 student
        DataStreamSource<Row> studentSource =  env.addSource(input);

        // 2. 过滤出年龄大于 16 的记录
        DataStream<Row> filtered = studentSource.filter(new FilterFunction<Row>() {
            @Override
            public boolean filter(Row value) throws Exception {

                return (int) value.getField(1) > 16;
            }
        });

        // 把结果输出到本地文件
        filtered.writeAsText("kafka-student.txt", FileSystem.WriteMode.OVERWRITE);

        // 3. 输出到 kafka 的 stuout 中
        filtered.addSink(output);

        // 触发运行
        env.execute("KafkaStreamApi");
    }
}
```
为了验证数据是否写入成功，稍微修改上述代码，从stuout中读取数据写入到本地文件，即可查看数据。

## Table Api & SQL 方式
记得加入table api相关依赖。

### 手动注册方式

```
import org.apache.flink.api.java.typeutils.RowTypeInfo;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.formats.json.JsonRowDeserializationSchema;
import org.apache.flink.formats.json.JsonRowSerializationSchema;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.connectors.kafka.KafkaTableSink;
import org.apache.flink.streaming.connectors.kafka.KafkaTableSource;
import org.apache.flink.streaming.connectors.kafka.config.StartupMode;
import org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner;
import org.apache.flink.table.api.DataTypes;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.TableSchema;
import org.apache.flink.table.api.java.StreamTableEnvironment;
import org.apache.flink.types.Row;

import java.util.Collections;
import java.util.Optional;
import java.util.Properties;

/**
 * kafka Table Api方式读写数据
 */
public class KafkaTableApi {

    public static void main(String[] args) throws Exception {
        // 运行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        // 使用Blink
        EnvironmentSettings bsSettings = EnvironmentSettings.newInstance()
                .useBlinkPlanner()
                .inStreamingMode()
                .build();
        // table 环境
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, bsSettings);

        // kafka 连接信息
        Properties properties = new Properties();
        properties.setProperty("bootstrap.servers", "127.0.0.1:9092");
        properties.setProperty("group.id", "KafkaTableApi");

        // 定义表结构
        TableSchema schema = TableSchema.builder()
                .field("name", DataTypes.STRING())
                .field("age", DataTypes.INT())
                .field("class", DataTypes.STRING())
                .build();

        // 1. 输入 这里的参数都是通过构造函数传入 使用起来较为复杂 推荐使用connect 和 ddl方式注册
        KafkaTableSource input = new KafkaTableSource(
                schema,
                Optional.empty(),
                Collections.emptyList(),
                Optional.empty(),
                "student",
                properties,
                new JsonRowDeserializationSchema
                        .Builder(
                        new RowTypeInfo(schema.getFieldTypes(), schema.getFieldNames()))
                        .failOnMissingField()
                        .build(),
                StartupMode.EARLIEST,
                Collections.emptyMap());

        // 注册
        tableEnv.registerTableSource("student", input);

        Optional<FlinkKafkaPartitioner<Row>> partitioner = Optional.empty();

        // 2. 输出
        KafkaTableSink output = new KafkaTableSink(
                schema,
                "stuout",
                properties,
                partitioner,
                new JsonRowSerializationSchema.Builder(new RowTypeInfo(
                        schema.getFieldTypes(), schema.getFieldNames()))
                        .build());
        // 注册
        tableEnv.registerTableSink("stuout", output);

        // 3. Table Api 方式 处理 数据 等同与下面的sql
        tableEnv
                .scan("student")
                .filter("age > 16")
                .insertInto("stuout");

        // 3. sql 方式处理数据
//        tableEnv.sqlUpdate("insert into stuout select name, age, class from student where age > 16");

        env.execute("KafkaTableApi");
    }
}

```
在创建Source和Sink时需要创建表结构，序列化和反序列化以及一些配置参数，使用起来较为复杂。

> KafkaTableSource 和 KafkaTableSink 为内部Api，不推荐直接使用，可以使用下面的 connect 和 ddl 方式注册输入和输入，然后使用sql和DataStream Api处理数据。


### 自动注册方式(ddl方式)

```
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.java.StreamTableEnvironment;
import org.apache.flink.table.descriptors.Json;
import org.apache.flink.table.descriptors.Kafka;
import org.apache.flink.table.descriptors.Schema;
import org.apache.flink.types.Row;

/**
 *  使用 connect 和 ddl 注册
 */
public class KafkaSql {

    public static void main(String[] args) throws Exception {
        // 运行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        // 使用Blink
        EnvironmentSettings bsSettings = EnvironmentSettings.newInstance()
                .useBlinkPlanner()
                .inStreamingMode()
                .build();

        // table 环境
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, bsSettings);


        String input = "CREATE TABLE student (\n" +
                "    name VARCHAR,\n" +
                "    age INT,\n" +
                "    class VARCHAR\n" +
                ") WITH (\n" +
                "    'connector.type' = 'kafka',\n" +
                "    'connector.version' = 'universal',\n" +
                "    'connector.topic' = 'student',\n" +
                "    'connector.startup-mode' = 'earliest-offset',\n" +
                "    'connector.properties.0.key' = 'bootstrap.servers',\n" +
                "    'connector.properties.0.value' = '127.0.0.1:9092',\n" +
                "    'connector.properties.1.key' = 'group.id',\n" +
                "    'connector.properties.1.value' = 'kafka/sql',\n" +
                "    'update-mode' = 'append',\n" +
                "    'format.type' = 'json',\n" +
                "    'format.derive-schema' = 'true'\n" +
                ")";

        // DDL 注册
        tableEnv.sqlUpdate(input);

        // 使用 connect 方式 把 stuout 注册为 输入 输出
        tableEnv.connect(new Kafka()
                    .version("universal")
                    .property("bootstrap.servers", "127.0.0.1:9092")
                    .property("group.id", "kafka-sql")
                    .topic("stuout")
                    .startFromEarliest())
                .withFormat(
                    new Json()
                    .deriveSchema())
                .withSchema(new Schema()
                    .field("name", "VARCHAR")
                    .field("age", "INT")
                    .field("class", "VARCHAR"))
                .inAppendMode()
                .registerTableSourceAndSink("stuout");

        // 把 Table 转为 DataStream
        // 转为DataStream 后 可以使用 DataStream Api 进行数据的处理
        // 把数据写入到本地
        tableEnv.toAppendStream(tableEnv.sqlQuery("select name,age ,class from student"), Row.class)
                .writeAsText("h2.txt");


        tableEnv.sqlUpdate("insert into stuout select name,age,class from student where age > 16");

        env.execute("kafka-sql");
    }

}
```

## 总结 

上述代码中使用 `ddl` 和 `connect` 方式描述输入输出信息，系统内部自动把该描述信息转换为手动注册方式中的创建Source、Sink和注册的过程。

创建 Source 和 Sink 的过程比较麻烦，推荐使用connect和ddl方式注册输入输出。


> 由于kafka中的消息是中的消息是不断流入的，所以运行上述程序时，程序是不会中断的一直在运行中。

> 在pom.xml中加入依赖
  ```
  <dependency>
	<groupId>org.apache.flink</groupId>
	<artifactId>flink-runtime-web_2.11</artifactId>
	<version>${flink.version}</version>
 </dependency>
  ```

> 把获取运行环境的代码修改为
  ```
   StreamExecutionEnvironment env = StreamExecutionEnvironment
                .createLocalEnvironmentWithWebUI(new Configuration());
  ```
  
> 程序启动后访问 http://localhost:8081 可以看到Flink的 Dashboard界面。
