# 1. Flink输入输出-csv

读取本地csv文件 经过简单的数据处理后 写入到本地csv文件。
 
---

在resources目录下 新建一个student.csv文件，内容如下
```
name,age,class
xiaoming,17,3-1
lilei,18,3-2
lucy,17,2-1
lily,15,2-2
```

读取student.csv文件，过滤出年龄大于16的记录写入到out.csv文件中。

## Datastream API 方式

```

import org.apache.flink.api.common.functions.FilterFunction;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.api.java.io.RowCsvInputFormat;
import org.apache.flink.api.java.tuple.Tuple3;
import org.apache.flink.core.fs.FileSystem;
import org.apache.flink.core.fs.Path;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.types.Row;


/**
 * 使用Stream API的方式读取 写入 本地csv文件
 * 输入文件格式为
 *      name,age,class
 *      string,int,string
 */
public class CsvStreamApi {

    public static void main(String[] args) throws Exception{

        String rootDir = CsvStreamApi.class.getResource("/").toURI().getPath();
        // 输入文件路径
        String inFilePath = rootDir+"/student.csv";
        // 输出文件路径 运行后在 target/classes 路径下
        String outFilePath = rootDir+"/out1.csv";

        // 获取运行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        // 设置并发度为1 可以不设
        env.setParallelism(1);

        // 使用 RowCsvInputFormat 把每一行记录解析为一个 Row
        RowCsvInputFormat csvInput = new RowCsvInputFormat(
                new Path(inFilePath),                                        // 文件路径
                new TypeInformation[]{Types.STRING, Types.INT, Types.STRING},// 字段类型
                "\n",                                             // 行分隔符
                ",");                                            // 字段分隔符
        // 跳过第一行 表头
        csvInput.setSkipFirstLineAsHeader(true);

        // 输入
        DataStreamSource<Row> student =  env.readFile(csvInput, inFilePath);

        // 过滤出年龄大于 16 的行
        DataStream<Row> filtered = student.filter(new FilterFunction<Row>() {
            @Override
            public boolean filter(Row value) throws Exception {
                Object obj = value.getField(1);
                if(obj == null) return false;
                return (int)obj > 16;
            }
        });

        // writeAsCsv 只能处理 Tuple 类型的数据 这里做一下转换，也可以输出为文本
        DataStream<Tuple3<String, Integer, String>> tuple3  = filtered.map(
                new MapFunction<Row, Tuple3<String,Integer, String>>() {
            @Override
            public Tuple3<String, Integer, String> map(Row value) throws Exception {
                return new Tuple3((String)value.getField(0),
                        (Integer)value.getField(1), (String) value.getField(2));
            }
        });

        // 输出到csv文件
        tuple3.writeAsCsv(outFilePath, FileSystem.WriteMode.OVERWRITE);

        // 触发执行
        env.execute();
    }
}
```

## Table API & SQL 方式
在pom.xml加入依赖
```
<dependency>
	<groupId>org.apache.flink</groupId>
	<artifactId>flink-table-planner-blink_2.11</artifactId>
	<version>${flink.version}</version>
</dependency>
```
代码如下
```
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.core.fs.FileSystem;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.java.StreamTableEnvironment;
import org.apache.flink.table.sinks.CsvTableSink;
import org.apache.flink.table.sinks.TableSink;
import org.apache.flink.table.sources.CsvTableSource;

/**
 * 使用TableAPI的方式读取 写入 本地csv文件 
 */
public class CsvTableApi {

    public static void main(String[] args) throws Exception {

        String rootDir = CsvStreamApi.class.getResource("/").toURI().getPath();
        // 输入文件路径
        String inFilePath = rootDir + "/student.csv";
        // 输出文件路径 运行后在 target/classes 路径下
        String outFilePath = rootDir + "/out.csv";

        // 运行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        // 使用Blink
        EnvironmentSettings bsSettings = EnvironmentSettings.newInstance()
                .useBlinkPlanner()
                .inStreamingMode()
                .build();
        // table 环境
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, bsSettings);


        // csv 输入源
        CsvTableSource csvTableSource = CsvTableSource.builder()
                .path(inFilePath)
                .fieldDelimiter(",")
                .ignoreFirstLine()
                .field("name", Types.STRING)
                .field("age", Types.INT)
                .field("class", Types.STRING)
                .build();

        // 注册为数据表
        tableEnv.registerTableSource("student", csvTableSource);

        // csv 输出
        TableSink csvTableSink = new CsvTableSink(outFilePath, ",", 1, FileSystem.WriteMode.OVERWRITE)
                .configure(new String[]{"name", "age", "class"},
                new TypeInformation[]{Types.STRING, Types.INT, Types.STRING});

        // 注册为数据表
        tableEnv.registerTableSink("stuout", csvTableSink);

        // 执行sql查询 查询后的数据输出到out中
        tableEnv.sqlQuery("select name,age,class from student where age > 16").insertInto("stuout");
        // 或者 下面的方法 把查询后的数据输出到out
//        tableEnv.sqlUpdate("insert into stuout (select name,age,class from student where age > 16)");

        env.execute();
    }
}
```

## 从hdfs中读写文件
pom.xml中加入下面的依赖
```
<dependency>
	<groupId>org.apache.flink</groupId>
	<artifactId>flink-shaded-hadoop-2-uber</artifactId>
	<version>2.7.5-8.0</version>
</dependency>
```
文件地址换为hdfs中文件地址即可，需要注意hdfs中文件的读写权限。