# 5. Flink输入输出-Elasticsearch
 
## 功能描述
Flink的官方代码中支持多个版本的Elasticsearch的写入。
官方代码中只有写数据到es中的连接器，无法读取数据。

我们先把前面的kafka中的student中的数据写入到elasticsearch中。
然后自己写一个从es中读取数据的连接器。


## 环境准备

flink-connector-elasticsearch6 支持 `elasticsearch6` 及以上版本。

pom.xml 依赖
```
<dependency>
	<groupId>org.apache.flink</groupId>
	<artifactId>flink-connector-elasticsearch6_${scala.binary.version}</artifactId>
	<version>${flink.version}</version>
</dependency>

<!-- flink-connector-elasticsearch6
依赖的httpclient的版本较高 在这里显示引入 防止冲突 -->
<dependency>
	<groupId>org.apache.httpcomponents</groupId>
	<artifactId>httpclient</artifactId>
	<version>4.5.2</version>
</dependency>
<dependency>
	<groupId>org.apache.httpcomponents</groupId>
	<artifactId>httpcore</artifactId>
	<version>4.4.5</version>
</dependency>
```

## DataStream Api方式
从kafka中读取数据的代码在前面的章节中已经给出，这里仅给出如何创建Elasticsearch 写入的相关代码片段。
```
    // 把结果输出到本地文件
    filtered.writeAsText("kafka-student.txt", FileSystem.WriteMode.OVERWRITE);
    
    ElasticsearchSink.Builder<Row> esOut =  new ElasticsearchSink.Builder<Row>(
            Lists.newArrayList(new HttpHost("127.0.0.1", 9200)),
            new ElasticsearchSinkFunction<Row>() {
                @Override
                public void process(Row row, RuntimeContext runtimeContext, RequestIndexer requestIndexer) {
                    Map<String, Object> map = new HashMap<>();
                    map.put("name", row.getField(0));
                    map.put("age", row.getField(1));
                    map.put("class", row.getField(2));
                    requestIndexer.add(new IndexRequest("stuout", "_doc").source(map));
                }
            }
    );
    esOut.setBulkFlushInterval(200);


    // 3. 输出到 kafka 的 stuout 中
    filtered.addSink(esOut.build());
```

## Table Api & SQL 方式
使用ddl方式描述输出，代码片段。
```
    String es = "CREATE TABLE stuout (\n" +
            "    name VARCHAR,\n" +
            "    age INT,\n" +
            "    class VARCHAR\n" +
            ") WITH (\n" +
            "    'connector.type' = 'elasticsearch',\n" +
            "    'connector.version' = '6',\n" +
            "    'connector.document-type' = 'json',\n" +
            "    'connector.hosts.0.hostname' = '127.0.0.1',\n" +
            "    'connector.hosts.0.port' = '9200',\n" +
            "    'connector.hosts.0.protocol' = 'http',\n" +
            "    'connector.index' = 'stuout',\n" +
            "    'connector.bulk-flush.interval' = '200',\n" +
            "    'update-mode' = 'append',\n" +
            "    'format.type' = 'json',\n" +
            "    'format.derive-schema' = 'true'\n" +
            ")";

    // 注册
    tableEnv.sqlUpdate(es);
```

## 实现一个简单的 Elasticsearch 数据读取连接器
参考 JDBCInputFormat的实现， 使用Elasticsearch的scroll api 读取数据。
这里很多配置项直接写在代码里了，实际使用中扩展该代码增加配置项。

```
import org.apache.commons.collections.map.CaseInsensitiveMap;
import org.apache.flink.annotation.PublicEvolving;
import org.apache.flink.api.common.io.DefaultInputSplitAssigner;
import org.apache.flink.api.common.io.RichInputFormat;
import org.apache.flink.api.common.io.statistics.BaseStatistics;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.java.typeutils.ResultTypeQueryable;
import org.apache.flink.api.java.typeutils.RowTypeInfo;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.core.io.GenericInputSplit;
import org.apache.flink.core.io.InputSplit;
import org.apache.flink.core.io.InputSplitAssigner;
import org.apache.flink.streaming.connectors.elasticsearch6.RestClientFactory;
import org.apache.flink.types.Row;
import org.apache.flink.util.Preconditions;
import org.apache.http.HttpHost;
import org.elasticsearch.action.search.ClearScrollRequest;
import org.elasticsearch.action.search.SearchRequest;
import org.elasticsearch.action.search.SearchResponse;
import org.elasticsearch.action.search.SearchScrollRequest;
import org.elasticsearch.client.RestClient;
import org.elasticsearch.client.RestClientBuilder;
import org.elasticsearch.client.RestHighLevelClient;
import org.elasticsearch.common.unit.TimeValue;
import org.elasticsearch.index.query.QueryBuilders;
import org.elasticsearch.search.Scroll;
import org.elasticsearch.search.builder.SearchSourceBuilder;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import java.io.IOException;
import java.util.Arrays;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

/**
 * Elasticsearch 数据读取
 */
public class ElasticsearchInput extends RichInputFormat<Row, InputSplit> implements ResultTypeQueryable<Row> {

    private static final Logger LOG = LoggerFactory.getLogger(ElasticsearchInput.class);

    private final List<HttpHost> httpHosts;
    private final RestClientFactory restClientFactory;
    private final String index;

    private transient RestHighLevelClient client;

    private String scrollId;
    private SearchRequest searchRequest;

    private RowTypeInfo rowTypeInfo;

    private boolean hasNext;

    private Iterator<Map<String, Object>> iterator;
    private Map<String, Integer> position;


    public ElasticsearchInput(List<HttpHost> httpHosts,
                              RestClientFactory restClientFactory, String index) {
        this.httpHosts = httpHosts;
        this.restClientFactory = restClientFactory;
        this.index = index;
    }

    @Override
    public void configure(Configuration parameters) {

    }

    @Override
    public BaseStatistics getStatistics(BaseStatistics cachedStatistics) throws IOException {
        return cachedStatistics;
    }

    @Override
    public InputSplit[] createInputSplits(int minNumSplits) throws IOException {
        return new GenericInputSplit[]{new GenericInputSplit(0, 1)};
    }

    @Override
    public InputSplitAssigner getInputSplitAssigner(InputSplit[] inputSplits) {
        return new DefaultInputSplitAssigner(inputSplits);
    }

    @Override
    public void open(InputSplit split) throws IOException {
        search();
    }

    // 从 es 中获取数据
    protected void search() throws IOException{
        SearchResponse searchResponse;
        if(scrollId == null){
            searchResponse = client.search(searchRequest);
            scrollId = searchResponse.getScrollId();
        }else{
            searchResponse = client.searchScroll(new SearchScrollRequest(scrollId));
        }

        if(searchResponse == null || searchResponse.getHits().getTotalHits() < 1){
            hasNext = false;
            return;
        }

        hasNext = true;
        iterator =  Arrays.stream(searchResponse.getHits().getHits())
                .map(t -> t.getSourceAsMap())
                .collect(Collectors.toList()).iterator();
    }



    @Override
    public void openInputFormat() throws IOException {
        RestClientBuilder builder = RestClient.builder(httpHosts.toArray(new HttpHost[httpHosts.size()]));
        restClientFactory.configureRestClientBuilder(builder);
        client = new RestHighLevelClient(builder);
        
        position = new CaseInsensitiveMap();
        int i = 0;
        for(String name : rowTypeInfo.getFieldNames()){
            position.put(name, i++);
        }

        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
        searchSourceBuilder.query(QueryBuilders.matchAllQuery());
        // 由于 测试数据比较少 所以这里的批次大小设置的比较小
        searchSourceBuilder.size(50);

        searchSourceBuilder.fetchSource(rowTypeInfo.getFieldNames(), null);

        // 使用 scroll api 获取数据
        Scroll scroll = new Scroll(TimeValue.timeValueMinutes(3l));

        searchRequest = new SearchRequest();
        searchRequest.indices(index);
        searchRequest.scroll(scroll);
        searchRequest.source(searchSourceBuilder);
    }

    @Override
    public boolean reachedEnd() throws IOException {
        return !hasNext;
    }

    @Override
    public Row nextRecord(Row reuse) throws IOException {
        if(!hasNext) return null;

        if(!iterator.hasNext()){
            this.search();
            if(!hasNext || !iterator.hasNext()){
                hasNext = false;
                return null;
            }
        }

        for(Map.Entry<String, Object> entry: iterator.next().entrySet()){
            Integer p = position.get(entry.getKey());
            if(p == null) throw new IOException("unknown field "+entry.getKey());

            reuse.setField(p, entry.getValue());
        }

        return reuse;
    }

    @Override
    public void close() throws IOException {
        if(client == null)
            return;

        iterator = null;
        ClearScrollRequest clearScrollRequest = new ClearScrollRequest();
        clearScrollRequest.addScrollId(scrollId);
        client.clearScroll(clearScrollRequest).isSucceeded();

        client.close();
        client = null;
    }

    @Override
    public TypeInformation<Row> getProducedType() {
        return rowTypeInfo;
    }

    public static Builder builder(List<HttpHost> httpHosts, String index){
        return new Builder(httpHosts, index);
    }

    @PublicEvolving
    public static class Builder {
        private final List<HttpHost> httpHosts;
        private String index;
        private RowTypeInfo rowTypeInfo;
        private RestClientFactory restClientFactory = restClientBuilder -> {
        };

        public Builder(List<HttpHost> httpHosts, String index) {
            this.httpHosts = Preconditions.checkNotNull(httpHosts);
            this.index = index;
        }


        public Builder setRowTypeInfo(RowTypeInfo rowTypeInfo) {
            this.rowTypeInfo = rowTypeInfo;
            return this;
        }


        public ElasticsearchInput build() {
            Preconditions.checkNotNull(this.rowTypeInfo);
            ElasticsearchInput input =  new ElasticsearchInput(httpHosts,  restClientFactory, index);
            input.rowTypeInfo = this.rowTypeInfo;
            return input;
        }

    }
}
```

## 读数据
从elasticsearch读取数据写到本地的代码如下
```
import com.google.common.collect.Lists;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.api.java.typeutils.RowTypeInfo;
import org.apache.flink.core.fs.FileSystem;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.http.HttpHost;


/**
 * 读elasticsearch 写到本地 
 */
public class ElasticsearchStreamApi {

    public static void main(String[] args) throws Exception{
        // 获取运行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        // 设置并发度为1 可以不设
        env.setParallelism(1);

        RowTypeInfo rowTypeInfo = new RowTypeInfo(
                new TypeInformation[]{Types.STRING, Types.INT, Types.STRING},
                new String[]{"name", "age", "class"});

        ElasticsearchInput es =  ElasticsearchInput.builder(
                Lists.newArrayList(new HttpHost("127.0.0.1", 9200)),
                "student")
                .setRowTypeInfo(rowTypeInfo)
                .build();

        // 输入
        DataStreamSource source = env.createInput(es);

        // 把结果输出到本地文件
        source.writeAsText("es-student.txt", FileSystem.WriteMode.OVERWRITE);

        // 触发运行
        env.execute("esStreamApi");
    }
    
}
```